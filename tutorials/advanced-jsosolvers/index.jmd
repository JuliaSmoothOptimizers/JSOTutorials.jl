---
title: "Comparing subsolvers for NLS solvers"
tags: ["solvers", "krylov", "benchmark", "least squares"]
author: "Tangi Migot"
---

# Comparing subsolvers for nonlinear least squares in JSOSolvers

This tutorial showcases some advanced features of solvers in JSOSolvers.

```julia
using JSOSolvers
```

Migration note (JSOSolvers ≥ 0.11): subsolvers are now selected with a Symbol via the `subsolver` keyword. Replace any old usages like `trunk(nlp, subsolver_type = CglsSolver)` with `trunk(nlp; subsolver = :cgls)`. See `JSOSolvers.trunkls_allowed_subsolvers` and `JSOSolvers.tronls_allowed_subsolvers` for the list of accepted symbols.

We benchmark different subsolvers used in the solver TRUNK for unconstrained nonlinear least squares problems.
The first step is to select a set of problems that are nonlinear least squares.

```julia
using ADNLPModels
using OptimizationProblems
using OptimizationProblems.ADNLPProblems
df = OptimizationProblems.meta
names = df[(df.objtype .== :least_squares) .& (df.contype .== :unconstrained), :name]
ad_problems = (eval(Meta.parse(problem))(use_nls = true) for problem ∈ names)
```

These problems are [`ADNLSModel`](https://github.com/JuliaSmoothOptimizers/ADNLPModels.jl) so derivatives are generated using automatic differentiation.

```julia
nls = first(ad_problems)
typeof(nls)
```

The solvers TRON and TRUNK are trust-region based methods that compute a search direction by means of solving iteratively a linear least squares problem.
For this task, several solvers are available.

```julia
JSOSolvers.trunkls_allowed_subsolvers
```

This benchmark could also be followed for the solver TRON where the following subsolvers are available.

```julia
JSOSolvers.tronls_allowed_subsolvers
```

These linear least squares solvers are implemented in the package [Krylov.jl](https://github.com/JuliaSmoothOptimizers/Krylov.jl).

```julia
using Krylov
```

For example, to call TRUNK with an explicit subsolver and tolerances:

```julia
stats = trunk(nls; subsolver = :cgls)
```

We define a dictionary of the different solvers that will be benchmarked.
We consider here four variants of TRUNK using the different subsolvers.

The same subsolver selection pattern applies to TRON’s least-squares specialization:

```julia
stats_tron = tron(nls; subsolver = :lsmr, rtol = RTOL, atol = ATOL, max_time = max_time)
```

```julia
solvers = Dict(
    :trunk_cgls => nlp -> trunk(nlp; subsolver = :cgls),
    :trunk_crls => nlp -> trunk(nlp; subsolver = :crls),
    :trunk_lsqr => nlp -> trunk(nlp; subsolver = :lsqr),
    :trunk_lsmr => nlp -> trunk(nlp; subsolver = :lsmr)
)
```

If needed, you can also pass tolerances and runtime limits directly to `trunk` while selecting the subsolver:

```julia
RTOL = 1e-6; ATOL = 1e-8; max_time = 60.0
solvers = Dict(
    :trunkls_lsmr => nlp -> trunk(nlp; rtol = RTOL, atol = ATOL, subsolver = :lsmr, max_time = max_time),
    :trunkls_cg   => nlp -> trunk(nlp; rtol = RTOL, atol = ATOL, subsolver = :cg,   max_time = max_time),
)
```

Using [`SolverBenchmark.jl`](https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl) functionalities, the solvers are executed over all the test problems.

```julia
using SolverBenchmark
stats = bmark_solvers(solvers, ad_problems)
```

The result is stored in a dictionary of `DataFrame` that can be used to analyze the results.

```julia
first_order(df) = df.status .== :first_order
unbounded(df) = df.status .== :unbounded
solved(df) = first_order(df) .| unbounded(df)

costnames = ["time"]
costs = [df -> .!solved(df) .* Inf .+ df.elapsed_time]
```

We compare the four variants based on their execution time.
More advanced comparisons could include the number of evaluations of the objective, gradient, or Hessian-vector products.

```julia
using Plots
gr()

profile_solvers(stats, costs, costnames)
```

The CRLS and CGLS variants are the ones solving more problems, and even though the difference is rather small the CGLS variant is consistently faster which seems to indicate that it is the most appropriate subsolver for TRUNK.
The size of the problems was rather small here, so this should be confirmed on larger instances.
Moreover, the results may vary depending on the origin of the test problems.
